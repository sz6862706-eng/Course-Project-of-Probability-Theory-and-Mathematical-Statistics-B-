import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch.nn as nn
import torch
import numpy as np
import pandas as pd
from torch.utils.data import DataLoader, Dataset
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt
from models.model import RNNModel, StockDataset, SeriesRNNModel, ParallelRNNModel
import time

class EarlyStopping:
    def __init__(self, patience=5, min_delta=0, verbose=True):
        self.patience = patience
        self.min_delta = min_delta
        self.verbose = verbose
        self.counter = 0
        self.best_loss = None
        self.early_stop = False
        self.best_model_state = None
        
    def __call__(self, val_loss, model):
        if self.best_loss is None:
            self.best_loss = val_loss
            self.best_model_state = model.state_dict().copy()
            if self.verbose:
                print(f'åˆå§‹éªŒè¯æŸå¤±: {val_loss:.6f}')
        elif val_loss > self.best_loss - self.min_delta:
            self.counter += 1
            if self.verbose:
                print(f'æ—©åœè®¡æ•°: {self.counter}/{self.patience}')
            if self.counter >= self.patience:
                self.early_stop = True
                if self.verbose:
                    print('è§¦å‘æ—©åœæœºåˆ¶!')
        else:
            if self.verbose:
                print(f'éªŒè¯æŸå¤±æ”¹å–„: {self.best_loss:.6f} -> {val_loss:.6f}')
            self.best_loss = val_loss
            self.best_model_state = model.state_dict().copy()
            self.counter = 0

def calculate_metrics(model, data_loader, device, scaler):
    """è®¡ç®—MSEã€MAEå’ŒRMSEæŒ‡æ ‡(åœ¨åŸå§‹å°ºåº¦ä¸Š)"""
    model.eval()
    all_preds = []
    all_targets = []
    
    with torch.no_grad():
        for x, y in data_loader:
            x, y = x.to(device), y.to(device)
            pred = model(x)
            all_preds.append(pred.cpu().numpy())
            all_targets.append(y.cpu().numpy())
    
    all_preds = np.concatenate(all_preds, axis=0)
    all_targets = np.concatenate(all_targets, axis=0)
    
    # è®¡ç®—æŒ‡æ ‡
    mse = mean_squared_error(all_targets, all_preds)
    mae = mean_absolute_error(all_targets, all_preds)
    rmse = np.sqrt(mse)
    
    # è®¡ç®—MAPE (Mean Absolute Percentage Error)
    mape = np.mean(np.abs((all_targets - all_preds) / all_targets)) * 100
    
    return mse, mae, rmse, mape, all_preds, all_targets

def calculate_baseline_metrics(test_targets_original):
    """
    è®¡ç®—åŸºçº¿æ¨¡å‹çš„æ€§èƒ½ (ä½¿ç”¨ç®€å•çš„æŒä¹…æ€§æ¨¡å‹)
    æŒä¹…æ€§æ¨¡å‹: é¢„æµ‹å€¼ = å‰ä¸€ä¸ªæ—¶é—´æ­¥çš„çœŸå®å€¼
    """
    # ç®€å•é¢„æµ‹: ç”¨å‰ä¸€ä¸ªå€¼ä½œä¸ºé¢„æµ‹
    baseline_preds = test_targets_original[:-1]
    baseline_targets = test_targets_original[1:]
    
    baseline_mse = mean_squared_error(baseline_targets, baseline_preds)
    baseline_mae = mean_absolute_error(baseline_targets, baseline_preds)
    baseline_rmse = np.sqrt(baseline_mse)
    baseline_mape = np.mean(np.abs((baseline_targets - baseline_preds) / baseline_targets)) * 100
    
    return baseline_mse, baseline_mae, baseline_rmse, baseline_mape

def train_and_evaluate(model, model_name, train_loader, val_loader, test_loader, 
                       device, scaler, epochs=100, lr=0.001, patience=5):
    """è®­ç»ƒå¹¶è¯„ä¼°å•ä¸ªæ¨¡å‹"""
    print(f"\n{'='*60}")
    print(f"å¼€å§‹è®­ç»ƒ: {model_name}")
    print(f"{'='*60}")
    
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = nn.MSELoss()
    early_stopping = EarlyStopping(patience=patience, min_delta=1e-4, verbose=True)
    
    train_losses = []
    val_losses = []
    start_time = time.time()
    
    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0
        for x, y in train_loader:
            x, y = x.to(device), y.to(device)
            optimizer.zero_grad()
            pred = model(x)
            loss = criterion(pred, y)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
        
        avg_train_loss = train_loss / len(train_loader)
        train_losses.append(avg_train_loss)
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for x, y in val_loader:
                x, y = x.to(device), y.to(device)
                pred = model(x)
                loss = criterion(pred, y)
                val_loss += loss.item()
        
        avg_val_loss = val_loss / len(val_loader)
        val_losses.append(avg_val_loss)
        
        print(f"Epoch {epoch+1}/{epochs}, Train Loss = {avg_train_loss:.6f}, Val Loss = {avg_val_loss:.6f}")
        
        # æ—©åœæ£€æŸ¥
        early_stopping(avg_val_loss, model)
        
        if early_stopping.early_stop:
            print(f"åœ¨ç¬¬ {epoch+1} è½®è§¦å‘æ—©åœ")
            model.load_state_dict(early_stopping.best_model_state)
            break
    
    training_time = time.time() - start_time
    
    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°(å½’ä¸€åŒ–æŸå¤±)
    model.eval()
    test_loss = 0
    with torch.no_grad():
        for x, y in test_loader:
            x, y = x.to(device), y.to(device)
            pred = model(x)
            loss = criterion(pred, y)
            test_loss += loss.item()
    
    avg_test_loss = test_loss / len(test_loader)
    
    # è®¡ç®—MSEã€MAEã€RMSEå’ŒMAPE(åŸå§‹å°ºåº¦)
    test_mse, test_mae, test_rmse, test_mape, preds, targets = calculate_metrics(
        model, test_loader, device, scaler
    )
    
    # ä¿å­˜æ¨¡å‹
    model_path = f'models/{model_name}_best.pth'
    os.makedirs('models', exist_ok=True)
    torch.save(model.state_dict(), model_path)
    
    print(f"\n{model_name} è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ä½³éªŒè¯æŸå¤±(å½’ä¸€åŒ–): {early_stopping.best_loss:.6f}")
    print(f"æµ‹è¯•é›†æŸå¤±(å½’ä¸€åŒ–): {avg_test_loss:.6f}")
    print(f"æµ‹è¯•é›†MSE(åŸå§‹å°ºåº¦): {test_mse:.2f}")
    print(f"æµ‹è¯•é›†RMSE(åŸå§‹å°ºåº¦): {test_rmse:.2f}")
    print(f"æµ‹è¯•é›†MAE(åŸå§‹å°ºåº¦): {test_mae:.2f}")
    print(f"æµ‹è¯•é›†MAPE: {test_mape:.2f}%")
    print(f"è®­ç»ƒæ—¶é—´: {training_time:.2f}ç§’")
    print(f"æ¨¡å‹å·²ä¿å­˜è‡³: {model_path}")
    
    return {
        'model_name': model_name,
        'train_losses': train_losses,
        'val_losses': val_losses,
        'best_val_loss': early_stopping.best_loss,
        'test_loss': avg_test_loss,
        'test_mse': test_mse,
        'test_rmse': test_rmse,
        'test_mae': test_mae,
        'test_mape': test_mape,
        'training_time': training_time,
        'num_epochs': len(train_losses),
        'predictions': preds,
        'targets': targets
    }

# ä¸»ç¨‹åº
if __name__ == "__main__":
    # è¯»å–æ•°æ®
    data_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 
                             'dataset', 'SSE 50.csv')
    df = pd.read_csv(data_path)
    close = df["Close"].values.reshape(-1, 1)
    
    print(f"åŸå§‹æ•°æ®ç»Ÿè®¡:")
    print(f"æ•°æ®ç‚¹æ•°: {len(close)}")
    print(f"æœ€å°å€¼: {close.min():.2f}")
    print(f"æœ€å¤§å€¼: {close.max():.2f}")
    print(f"å‡å€¼: {close.mean():.2f}")
    print(f"æ ‡å‡†å·®: {close.std():.2f}")
    
    torch.manual_seed(2025)
    np.random.seed(2025)
    
    # ğŸ”§ ä¿®å¤1: å…ˆåˆ’åˆ†æ•°æ®,å†å½’ä¸€åŒ–
    # åˆ’åˆ†æ•°æ®é›† (70% è®­ç»ƒ, 10% éªŒè¯, 20% æµ‹è¯•)
    train_size = int(len(close) * 0.7)
    val_size = int(len(close) * 0.1)
    
    train_close = close[:train_size]
    val_close = close[train_size:train_size + val_size]
    test_close = close[train_size + val_size:]
    
    # ğŸ”§ ä¿®å¤2: åªåœ¨è®­ç»ƒé›†ä¸Šfit scaler
    scaler = MinMaxScaler()
    train_data = scaler.fit_transform(train_close)  # åªåœ¨è®­ç»ƒé›†ä¸Šfit
    val_data = scaler.transform(val_close)          # ç”¨è®­ç»ƒé›†çš„scaler transform
    test_data = scaler.transform(test_close)        # ç”¨è®­ç»ƒé›†çš„scaler transform
    
    print(f"\næ•°æ®é›†åˆ’åˆ†:")
    print(f"è®­ç»ƒé›†: {len(train_data)} ({len(train_data)/(len(train_data)+len(val_data)+len(test_data))*100:.1f}%)")
    print(f"éªŒè¯é›†: {len(val_data)} ({len(val_data)/(len(train_data)+len(val_data)+len(test_data))*100:.1f}%)")
    print(f"æµ‹è¯•é›†: {len(test_data)} ({len(test_data)/(len(train_data)+len(val_data)+len(test_data))*100:.1f}%)")
    
    print(f"\nå½’ä¸€åŒ–åçš„æ•°æ®èŒƒå›´:")
    print(f"è®­ç»ƒé›†: [{train_data.min():.4f}, {train_data.max():.4f}]")
    print(f"éªŒè¯é›†: [{val_data.min():.4f}, {val_data.max():.4f}]")
    print(f"æµ‹è¯•é›†: [{test_data.min():.4f}, {test_data.max():.4f}]")
    
    # ğŸ”§ ä¿®å¤3: ä½¿ç”¨æ›´åˆç†çš„åºåˆ—é•¿åº¦
    seq_len = 20  # å‡å°‘åˆ°20,é¿å…æ¨¡å‹åªéœ€è¦è®°ä½æœ€åä¸€ä¸ªå€¼
    
    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    train_dataset = StockDataset(train_data, seq_len)
    val_dataset = StockDataset(val_data, seq_len)
    test_dataset = StockDataset(test_data, seq_len)
    
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"\nä½¿ç”¨è®¾å¤‡: {device}")
    print(f"åºåˆ—é•¿åº¦: {seq_len}")
    
    # è®¡ç®—åŸºçº¿æ¨¡å‹æ€§èƒ½
    print(f"\n{'='*60}")
    print("åŸºçº¿æ¨¡å‹ (æŒä¹…æ€§æ¨¡å‹) æ€§èƒ½:")
    print(f"{'='*60}")
    baseline_mse, baseline_mae, baseline_rmse, baseline_mape = calculate_baseline_metrics(test_close)
    print(f"åŸºçº¿MSE: {baseline_mse:.2f}")
    print(f"åŸºçº¿RMSE: {baseline_rmse:.2f}")
    print(f"åŸºçº¿MAE: {baseline_mae:.2f}")
    print(f"åŸºçº¿MAPE: {baseline_mape:.2f}%")
    print(f"{'='*60}")
    
    # å®šä¹‰è¦å®éªŒçš„æ¨¡å‹
    models_to_test = [
        ("RNN", RNNModel(rnn_type="RNN").to(device)),
        ("LSTM", RNNModel(rnn_type="LSTM").to(device)),
        ("GRU", RNNModel(rnn_type="GRU").to(device)),
        ("Series_RNN", SeriesRNNModel(hidden_size=64, num_layers=2).to(device)),
        ("Parallel_RNN_Concat", ParallelRNNModel(hidden_size=64, num_layers=2, fusion_method="concat").to(device)),
        ("Parallel_RNN_Attention", ParallelRNNModel(hidden_size=64, num_layers=2, fusion_method="attention").to(device)),
    ]
    
    # è®­ç»ƒæ‰€æœ‰æ¨¡å‹å¹¶æ”¶é›†ç»“æœ
    results = []
    for model_name, model in models_to_test:
        result = train_and_evaluate(
            model=model,
            model_name=model_name,
            train_loader=train_loader,
            val_loader=val_loader,
            test_loader=test_loader,
            device=device,
            scaler=scaler,
            epochs=100,
            lr=0.001,
            patience=20
        )
        results.append(result)
    
    # ä¿å­˜ç»“æœåˆ°CSV
    results_df = pd.DataFrame([
        {
            'Model': r['model_name'],
            'Best Val Loss': r['best_val_loss'],
            'Test Loss': r['test_loss'],
            'Test MSE': r['test_mse'],
            'Test RMSE': r['test_rmse'],
            'Test MAE': r['test_mae'],
            'Test MAPE (%)': r['test_mape'],
            'Training Time (s)': r['training_time'],
            'Num Epochs': r['num_epochs'],
            'MSE vs Baseline': f"{(r['test_mse']/baseline_mse):.2%}",
            'MAE vs Baseline': f"{(r['test_mae']/baseline_mae):.2%}"
        }
        for r in results
    ])
    results_df.to_csv('experiment_results_fixed.csv', index=False)
    
    print(f"\n{'='*60}")
    print("å®éªŒç»“æœæ±‡æ€» (ä¿®å¤æ•°æ®æ³„éœ²å):")
    print(f"{'='*60}")
    print(results_df.to_string(index=False))
    print(f"\nç»“æœå·²ä¿å­˜è‡³: experiment_results_fixed.csv")
    
    # ç»˜åˆ¶å¯¹æ¯”å›¾
    fig = plt.figure(figsize=(20, 12))
    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)
    
    # 1. è®­ç»ƒæŸå¤±å¯¹æ¯”
    ax1 = fig.add_subplot(gs[0, 0])
    for r in results:
        ax1.plot(r['train_losses'], label=r['model_name'], alpha=0.7)
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Train Loss')
    ax1.set_title('Training Loss Comparison')
    ax1.legend()
    ax1.grid(True)
    
    # 2. éªŒè¯æŸå¤±å¯¹æ¯”
    ax2 = fig.add_subplot(gs[0, 1])
    for r in results:
        ax2.plot(r['val_losses'], label=r['model_name'], alpha=0.7)
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Validation Loss')
    ax2.set_title('Validation Loss Comparison')
    ax2.legend()
    ax2.grid(True)
    
    # 3. MSEå¯¹æ¯” (åŒ…å«åŸºçº¿)
    ax3 = fig.add_subplot(gs[0, 2])
    model_names = ['Baseline'] + [r['model_name'] for r in results]
    test_mses = [baseline_mse] + [r['test_mse'] for r in results]
    colors = ['red'] + ['steelblue']*len(results)
    bars = ax3.bar(range(len(model_names)), test_mses, color=colors, alpha=0.7)
    ax3.set_xticks(range(len(model_names)))
    ax3.set_xticklabels(model_names, rotation=45, ha='right')
    ax3.set_ylabel('MSE (Original Scale)')
    ax3.set_title('Test MSE Comparison (with Baseline)')
    ax3.grid(True, axis='y')
    for i, bar in enumerate(bars):
        height = bar.get_height()
        ax3.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.2f}', ha='center', va='bottom', fontsize=8)
    
    # 4. RMSEå¯¹æ¯”
    ax4 = fig.add_subplot(gs[1, 0])
    test_rmses = [baseline_rmse] + [r['test_rmse'] for r in results]
    bars = ax4.bar(range(len(model_names)), test_rmses, color=colors, alpha=0.7)
    ax4.set_xticks(range(len(model_names)))
    ax4.set_xticklabels(model_names, rotation=45, ha='right')
    ax4.set_ylabel('RMSE (Original Scale)')
    ax4.set_title('Test RMSE Comparison (with Baseline)')
    ax4.grid(True, axis='y')
    for i, bar in enumerate(bars):
        height = bar.get_height()
        ax4.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.2f}', ha='center', va='bottom', fontsize=8)
    
    # 5. MAEå¯¹æ¯”
    ax5 = fig.add_subplot(gs[1, 1])
    test_maes = [baseline_mae] + [r['test_mae'] for r in results]
    bars = ax5.bar(range(len(model_names)), test_maes, color=colors, alpha=0.7)
    ax5.set_xticks(range(len(model_names)))
    ax5.set_xticklabels(model_names, rotation=45, ha='right')
    ax5.set_ylabel('MAE (Original Scale)')
    ax5.set_title('Test MAE Comparison (with Baseline)')
    ax5.grid(True, axis='y')
    for i, bar in enumerate(bars):
        height = bar.get_height()
        ax5.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.2f}', ha='center', va='bottom', fontsize=8)
    
    # 6. MAPEå¯¹æ¯”
    ax6 = fig.add_subplot(gs[1, 2])
    test_mapes = [baseline_mape] + [r['test_mape'] for r in results]
    bars = ax6.bar(range(len(model_names)), test_mapes, color=colors, alpha=0.7)
    ax6.set_xticks(range(len(model_names)))
    ax6.set_xticklabels(model_names, rotation=45, ha='right')
    ax6.set_ylabel('MAPE (%)')
    ax6.set_title('Test MAPE Comparison (with Baseline)')
    ax6.grid(True, axis='y')
    for i, bar in enumerate(bars):
        height = bar.get_height()
        ax6.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.2f}%', ha='center', va='bottom', fontsize=8)
    
    # 7. é¢„æµ‹vsçœŸå®å€¼ (æœ€ä½³æ¨¡å‹)
    best_model_idx = np.argmin([r['test_mse'] for r in results])
    best_result = results[best_model_idx]
    
    ax7 = fig.add_subplot(gs[2, :2])
    sample_size = min(200, len(best_result['targets']))
    ax7.plot(best_result['targets'][:sample_size], label='True Values', alpha=0.7, linewidth=2)
    ax7.plot(best_result['predictions'][:sample_size], label='Predictions', alpha=0.7, linewidth=2)
    ax7.set_xlabel('Time Step')
    ax7.set_ylabel('Stock Price')
    ax7.set_title(f'Predictions vs True Values ({best_result["model_name"]})')
    ax7.legend()
    ax7.grid(True)
    
    # 8. è®­ç»ƒæ—¶é—´å¯¹æ¯”
    ax8 = fig.add_subplot(gs[2, 2])
    training_times = [r['training_time'] for r in results]
    bars = ax8.bar(range(len(results)), training_times, color='plum', alpha=0.7)
    ax8.set_xticks(range(len(results)))
    ax8.set_xticklabels([r['model_name'] for r in results], rotation=45, ha='right')
    ax8.set_ylabel('Training Time (seconds)')
    ax8.set_title('Training Time Comparison')
    ax8.grid(True, axis='y')
    for i, bar in enumerate(bars):
        height = bar.get_height()
        ax8.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.1f}s', ha='center', va='bottom', fontsize=8)
    
    plt.savefig('experiment_comparison_fixed.png', dpi=300, bbox_inches='tight')
    print(f"å¯¹æ¯”å›¾å·²ä¿å­˜è‡³: experiment_comparison_fixed.png")
    plt.show()
    
    # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
    best_model = results[best_model_idx]
    print(f"\n{'='*60}")
    print(f"æœ€ä½³æ¨¡å‹: {best_model['model_name']}")
    print(f"æµ‹è¯•é›†MSE: {best_model['test_mse']:.2f} (åŸºçº¿: {baseline_mse:.2f}, æ”¹è¿›: {(1-best_model['test_mse']/baseline_mse)*100:.1f}%)")
    print(f"æµ‹è¯•é›†RMSE: {best_model['test_rmse']:.2f} (åŸºçº¿: {baseline_rmse:.2f})")
    print(f"æµ‹è¯•é›†MAE: {best_model['test_mae']:.2f} (åŸºçº¿: {baseline_mae:.2f}, æ”¹è¿›: {(1-best_model['test_mae']/baseline_mae)*100:.1f}%)")
    print(f"æµ‹è¯•é›†MAPE: {best_model['test_mape']:.2f}% (åŸºçº¿: {baseline_mape:.2f}%)")
    print(f"è®­ç»ƒæ—¶é—´: {best_model['training_time']:.2f}ç§’")
    print(f"{'='*60}")